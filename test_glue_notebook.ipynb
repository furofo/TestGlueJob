{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aac447d",
   "metadata": {},
   "source": [
    "# AWS Glue Testing Notebook\n",
    "\n",
    "This notebook demonstrates how to test and run your AWS Glue Python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PySpark and Glue Context\n",
    "from pyspark.sql import SparkSession\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "# Create Spark and Glue contexts\n",
    "spark = SparkSession.builder.appName(\"TestGlueJob\").getOrCreate()\n",
    "glueContext = GlueContext(spark)\n",
    "job = Job(glueContext)\n",
    "\n",
    "print(\"‚úÖ Spark and Glue contexts created successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52dd4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Run your Python file directly using exec\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add current directory to path so we can import our modules\n",
    "sys.path.append('/home/hadoop/workspace')\n",
    "\n",
    "# Set command line arguments for your script\n",
    "sys.argv = ['sample.py', '--JOB_NAME', 'my-test-job']\n",
    "\n",
    "# Read and execute the Python file\n",
    "try:\n",
    "    with open('/home/hadoop/workspace/sample.py', 'r') as f:\n",
    "        script_content = f.read()\n",
    "    \n",
    "    exec(script_content)\n",
    "    print(\"‚úÖ Script executed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running script: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Import your script as a module (if it has functions)\n",
    "# First, let's see what files are available\n",
    "import os\n",
    "print(\"Files in workspace:\")\n",
    "for file in os.listdir('/home/hadoop/workspace'):\n",
    "    if file.endswith('.py'):\n",
    "        print(f\"  üìÑ {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c774ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Use subprocess to run the script (like running from terminal)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, 'sample.py', '--JOB_NAME', 'my-test-job'],\n",
    "        cwd='/home/hadoop/workspace',\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    print(\"STDOUT:\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\")\n",
    "        print(result.stderr)\n",
    "        \n",
    "    print(f\"Return code: {result.returncode}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: Use spark-submit (recommended for Spark jobs)\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        ['spark-submit', 'sample.py', '--JOB_NAME', 'my-test-job'],\n",
    "        cwd='/home/hadoop/workspace',\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    print(\"STDOUT:\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\")\n",
    "        print(result.stderr)\n",
    "        \n",
    "    print(f\"Return code: {result.returncode}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reading your sample data\n",
    "try:\n",
    "    # Read the parquet file if it exists\n",
    "    df = spark.read.parquet('/home/hadoop/workspace/data/employees.parquet')\n",
    "    print(f\"‚úÖ Successfully read parquet file with {df.count()} rows\")\n",
    "    \n",
    "    # Show schema and sample data\n",
    "    df.printSchema()\n",
    "    df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not read parquet file: {e}\")\n",
    "    print(\"You may need to create sample data first by running create_sample_data.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30fe3a2",
   "metadata": {},
   "source": [
    "## Running Scripts from Terminal\n",
    "\n",
    "You can also open a terminal in Jupyter Lab and run:\n",
    "\n",
    "```bash\n",
    "# Direct Python execution\n",
    "python3 sample.py --JOB_NAME my-test-job\n",
    "\n",
    "# Using spark-submit (recommended)\n",
    "spark-submit sample.py --JOB_NAME my-test-job\n",
    "\n",
    "# Make executable and run\n",
    "chmod +x sample.py\n",
    "./sample.py --JOB_NAME my-test-job\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
