{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c26173c-0247-465a-a5b0-f5cee5b1a8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample parquet file created at: data/employees.parquet\n",
      "Sample data preview:\n",
      "   employee_id            name   department  salary   hire_date\n",
      "0            1        John Doe  Engineering   75000  2020-01-15\n",
      "1            2      Jane Smith    Marketing   65000  2019-03-20\n",
      "2            3     Bob Johnson        Sales   55000  2021-07-10\n",
      "3            4     Alice Brown  Engineering   80000  2018-11-05\n",
      "4            5  Charlie Wilson           HR   60000  2022-02-28\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create sample data\n",
    "sample_data = {\n",
    "    'employee_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['John Doe', 'Jane Smith', 'Bob Johnson', 'Alice Brown', 'Charlie Wilson'],\n",
    "    'department': ['Engineering', 'Marketing', 'Sales', 'Engineering', 'HR'],\n",
    "    'salary': [75000, 65000, 55000, 80000, 60000],\n",
    "    'hire_date': ['2020-01-15', '2019-03-20', '2021-07-10', '2018-11-05', '2022-02-28']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(sample_data)\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Save as parquet file\n",
    "df.to_parquet('data/employees.parquet', index=False)\n",
    "\n",
    "print(\"Sample parquet file created at: data/employees.parquet\")\n",
    "print(\"Sample data preview:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "075f1b8b-be6a-49e8-a5f7-07e6230b2dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using existing SparkContext\n",
      "INFO:__main__:Starting AWS Glue PySpark ETL Job (Notebook Mode)\n",
      "INFO:__main__:Reading parquet file from: data/employees.parquet\n",
      "INFO:__main__:Starting AWS Glue PySpark ETL Job (Notebook Mode)\n",
      "INFO:__main__:Reading parquet file from: data/employees.parquet\n",
      "INFO:__main__:Total records read: 5\n",
      "INFO:__main__:Schema:\n",
      "INFO:__main__:Sample data (first 10 rows):\n",
      "INFO:__main__:Total records read: 5\n",
      "INFO:__main__:Schema:\n",
      "INFO:__main__:Sample data (first 10 rows):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- hire_date: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Performing data transformations...\n",
      "INFO:__main__:Department Statistics:\n",
      "INFO:__main__:Department Statistics:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------+------+----------+\n",
      "|employee_id|          name| department|salary| hire_date|\n",
      "+-----------+--------------+-----------+------+----------+\n",
      "|          1|      John Doe|Engineering| 75000|2020-01-15|\n",
      "|          2|    Jane Smith|  Marketing| 65000|2019-03-20|\n",
      "|          3|   Bob Johnson|      Sales| 55000|2021-07-10|\n",
      "|          4|   Alice Brown|Engineering| 80000|2018-11-05|\n",
      "|          5|Charlie Wilson|         HR| 60000|2022-02-28|\n",
      "+-----------+--------------+-----------+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Salary Grade Distribution:\n",
      "INFO:__main__:Converting to Glue DynamicFrame...\n",
      "INFO:__main__:Converting to Glue DynamicFrame...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+----------+----------+\n",
      "| department|employee_count|avg_salary|max_salary|min_salary|\n",
      "+-----------+--------------+----------+----------+----------+\n",
      "|Engineering|             2|   77500.0|     80000|     75000|\n",
      "|         HR|             1|   60000.0|     60000|     60000|\n",
      "|  Marketing|             1|   65000.0|     65000|     65000|\n",
      "|      Sales|             1|   55000.0|     55000|     55000|\n",
      "+-----------+--------------+----------+----------+----------+\n",
      "\n",
      "+------------+-----+\n",
      "|salary_grade|count|\n",
      "+------------+-----+\n",
      "|        High|    2|\n",
      "|         Low|    1|\n",
      "|      Medium|    2|\n",
      "+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:DynamicFrame record count: 5\n",
      "INFO:__main__:DynamicFrame schema:\n",
      "INFO:__main__:ETL job completed successfully!\n",
      "INFO:__main__:Notebook ETL execution completed!\n",
      "INFO:__main__:DynamicFrame schema:\n",
      "INFO:__main__:ETL job completed successfully!\n",
      "INFO:__main__:Notebook ETL execution completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- employee_id: long\n",
      "|-- name: string\n",
      "|-- department: string\n",
      "|-- salary: long\n",
      "|-- hire_date: string\n",
      "|-- salary_grade: string\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():\n",
    "    # Get job arguments (if any) - for notebook we'll skip this\n",
    "    # args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "    \n",
    "    # Check if SparkContext already exists and reuse it\n",
    "    try:\n",
    "        # Try to get existing SparkContext\n",
    "        sc = SparkContext.getOrCreate()\n",
    "        logger.info(\"Using existing SparkContext\")\n",
    "    except:\n",
    "        # Create new SparkContext if none exists\n",
    "        sc = SparkContext()\n",
    "        logger.info(\"Created new SparkContext\")\n",
    "    \n",
    "    glueContext = GlueContext(sc)\n",
    "    spark = glueContext.spark_session\n",
    "    \n",
    "    # For notebook environment, we'll skip the Job initialization\n",
    "    # as it's mainly needed for AWS Glue job tracking\n",
    "    logger.info(\"Starting AWS Glue PySpark ETL Job (Notebook Mode)\")\n",
    "    \n",
    "    try:\n",
    "        # Path to the sample parquet file\n",
    "        input_path = \"data/employees.parquet\"\n",
    "        \n",
    "        logger.info(f\"Reading parquet file from: {input_path}\")\n",
    "        \n",
    "        # Read parquet file using Spark\n",
    "        df = spark.read.parquet(input_path)\n",
    "        \n",
    "        # Log basic information about the dataset\n",
    "        logger.info(f\"Total records read: {df.count()}\")\n",
    "        logger.info(\"Schema:\")\n",
    "        df.printSchema()\n",
    "        \n",
    "        # Show sample data\n",
    "        logger.info(\"Sample data (first 10 rows):\")\n",
    "        df.show(10)\n",
    "        \n",
    "        # Perform some basic transformations\n",
    "        logger.info(\"Performing data transformations...\")\n",
    "        \n",
    "        # Add a new column for salary grade\n",
    "        df_transformed = df.withColumn(\n",
    "            \"salary_grade\",\n",
    "            when(col(\"salary\") >= 70000, \"High\")\n",
    "            .when(col(\"salary\") >= 60000, \"Medium\")\n",
    "            .otherwise(\"Low\")\n",
    "        )\n",
    "        \n",
    "        # Group by department and calculate statistics\n",
    "        dept_stats = df_transformed.groupBy(\"department\") \\\n",
    "            .agg(\n",
    "                count(\"employee_id\").alias(\"employee_count\"),\n",
    "                avg(\"salary\").alias(\"avg_salary\"),\n",
    "                max(\"salary\").alias(\"max_salary\"),\n",
    "                min(\"salary\").alias(\"min_salary\")\n",
    "            ).orderBy(\"department\")\n",
    "        \n",
    "        logger.info(\"Department Statistics:\")\n",
    "        dept_stats.show()\n",
    "        \n",
    "        # Show salary grade distribution\n",
    "        salary_grade_dist = df_transformed.groupBy(\"salary_grade\") \\\n",
    "            .agg(count(\"employee_id\").alias(\"count\")) \\\n",
    "            .orderBy(\"salary_grade\")\n",
    "        \n",
    "        logger.info(\"Salary Grade Distribution:\")\n",
    "        salary_grade_dist.show()\n",
    "        \n",
    "        # Convert to Glue DynamicFrame for demonstration\n",
    "        logger.info(\"Converting to Glue DynamicFrame...\")\n",
    "        dynamic_frame = glueContext.create_dynamic_frame.from_rdd(\n",
    "            df_transformed.rdd, \n",
    "            \"transformed_employees\"\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"DynamicFrame record count: {dynamic_frame.count()}\")\n",
    "        logger.info(\"DynamicFrame schema:\")\n",
    "        dynamic_frame.printSchema()\n",
    "        \n",
    "        # You could write the results to another location here\n",
    "        # For now, we'll just log the completion\n",
    "        logger.info(\"ETL job completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in ETL job: {str(e)}\")\n",
    "        raise e\n",
    "    \n",
    "    logger.info(\"Notebook ETL execution completed!\")\n",
    "\n",
    "# Run the main function\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa2115-c214-45ee-b467-e77f03505d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49c604-35e0-4dc4-9f0e-f16740ea5fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
